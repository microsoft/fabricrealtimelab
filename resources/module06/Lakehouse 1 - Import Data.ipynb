{"cells":[{"cell_type":"markdown","id":"17df4242-a0ba-47a2-a7d9-4a3cd1fa0d8a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Lakehouse 1: Import Data\n","\n","This notebook will download additional historic information and merge it into the raw_stock_data table. This additional data will help provide more interesting reports and data science exploration.\n","\n","Before starting, the raw_stock_data table should be receiving data regularly from the Eventstream."]},{"cell_type":"code","execution_count":null,"id":"de95bc6c-b751-4253-84f9-edb05e05496e","metadata":{},"outputs":[],"source":["import datetime\n","from datetime import timedelta\n","from delta.tables import *\n","from pyspark.sql.functions import *\n","\n","# change table name if not raw_stock_data\n","targetTableName = 'raw_stock_data'\n","daysToImport = 30\n","\n","minDateInRaw = datetime.datetime.utcnow() # will be recalculated as needed"]},{"cell_type":"code","execution_count":null,"id":"6ebdef6f","metadata":{},"outputs":[],"source":["def create_raw_table_if_needed():\n","    spark.sql(f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {targetTableName} (\n","            timestamp TIMESTAMP\n","            ,price DOUBLE \n","            ,Symbol STRING\n","            )\n","        USING DELTA\n","        \"\"\")\n","\n","create_raw_table_if_needed()"]},{"cell_type":"code","execution_count":null,"id":"442fff2f-7379-4b2f-9767-32946cd8cbbb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# find earliest date in raw_stock_data table, or use today's date if none\n","\n","df_min_date = spark.sql(f\"SELECT coalesce(min(to_timestamp(timestamp)),current_date()) as minDate FROM {targetTableName}\")\n","minDateInRaw = df_min_date.first()[\"minDate\"]\n","print(f\"Min date in raw table: {minDateInRaw}\")\n","\n","# import data up until about 1 hour before the current data\n","historicalEndDate = (minDateInRaw + datetime.timedelta(hours=-1)).replace(microsecond=0)\n","historicalBeginDate = historicalEndDate + datetime.timedelta(days=-daysToImport)\n","\n","# # import data before current day\n","# historicalEndDate = minDateInRaw.replace(hour=0, minute=0, second=0, microsecond=0)\n","# historicalBeginDate = historicalEndDate + datetime.timedelta(days=-daysToImport)\n","\n","print(f'Historical import begin date: {historicalBeginDate}')\n","print(f'Historical import end date: {historicalEndDate}')"]},{"cell_type":"markdown","id":"cdb998e3-73cb-4f96-af09-f75cdedfedcd","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Download historical data\n","\n","The cells below will download and unzip historical data to the lakehouse unmanaged files."]},{"cell_type":"code","execution_count":null,"id":"160c1754","metadata":{},"outputs":[],"source":["class HistoryData:\n","    def __init__(self, file_uri, filename, year) -> None:\n","        self.file_uri = file_uri\n","        self.filename = filename\n","        self.year = year\n","\n","def getDownloadInfo(year):\n","    if year==2023:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2023.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=ledWmONUdRKvcpDumZHpLPqkrTLWu%2B9GrF0gMh5QK2c%3D',\n","            'stockhistory-2023.tgz',\n","            year)\n","    elif year==2024:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2024.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=TIFg2tvEww3rdTVNOKo5ef1xTx%2Bs0XAbdEARKGhOiX8%3D',\n","            'stockhistory-2024.tgz',\n","            year)\n","    elif year==2025:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2025.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=UB4QhOmsfwhPC0rE14wRJQxeiXXutHxm%2BOVnFA3xDFQ%3D',\n","            'stockhistory-2025.tgz',\n","            year)\n","    elif year==2026:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2026.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=l4tonO4SZfuCbHrheomO0WNkuYfyTTdfdNrcfu%2Fc7dU%3D',\n","            'stockhistory-2026.tgz',\n","            year)\n","    else:\n","        return None\n","\n"]},{"cell_type":"code","execution_count":null,"id":"71588929","metadata":{},"outputs":[],"source":["import os\n","import datetime\n","from datetime import timedelta\n","\n","LAKEHOUSE_FOLDER = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/stockhistory/raw\"\n","\n","TAR_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/tar/\"\n","CSV_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/csv/\"\n","\n","def downloadHistoryIfNotExists():\n","\n","    currYear = datetime.datetime.utcnow().year\n","\n","    if not os.path.exists(LAKEHOUSE_FOLDER):\n","        # add a lakehouse if the notebook has no default lakehouse\n","        # a new notebook will not link to any lakehouse by default\n","        raise FileNotFoundError(\n","            \"Lakehouse not found, please add a lakehouse for the notebook.\"\n","        )\n","    else:\n","        for year in range(currYear, currYear-2, -1):\n","            fileInfo = getDownloadInfo(year)\n","\n","            if (fileInfo is None):\n","                print(f'No file exists for {year}')\n","                continue\n","\n","            # verify if files are already in the lakehouse, and if not, download and unzip\n","            if not os.path.exists(f\"{TAR_FILE_PATH}{fileInfo.filename}\"):\n","                print(f'Downloading {fileInfo.filename}')\n","                os.makedirs(TAR_FILE_PATH, exist_ok=True)\n","                os.system(f\"wget '{fileInfo.file_uri}' -O {TAR_FILE_PATH}{fileInfo.filename}\")\n","\n","                #todo: better file checking\n","                os.makedirs(CSV_FILE_PATH, exist_ok=True)\n","                print(f'Extracting {fileInfo.filename}')\n","                os.system(f\"tar -zxvf {TAR_FILE_PATH}{fileInfo.filename} -C {CSV_FILE_PATH}\")\n","            else:\n","                print(f'File already exists: {fileInfo.filename}')\n","\n","downloadHistoryIfNotExists()"]},{"cell_type":"code","execution_count":null,"id":"f8137170","metadata":{},"outputs":[],"source":["# verify csv files are available\n","\n","import time\n","\n","path_to_check = f'{DATA_FOLDER}/csv'\n","files_found = False\n","check_count = 0\n","\n","while (files_found == False):\n","    try:\n","        check_count += 1\n","        files = mssparkutils.fs.ls(path_to_check)\n","        if (len(files) > 0):\n","            files_found = True\n","        print(f'Found {len(files)} CSV folders.')\n","    except Exception as e:\n","        if (check_count > 10):\n","            print('Unable to verify CSV files. Please restart session and verify files are downloading and extracting.')\n","            raise e\n","        print('Checking for files...')\n","        time.sleep(1)\n"]},{"cell_type":"code","execution_count":null,"id":"56efbae5-d607-450a-8cc3-a96d57e7504d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# read the CSV files, {year}/{month}/{day}.csv\n","\n","from pyspark.sql import types as T\n","\n","schema = T.StructType([\n","    T.StructField(\"price\", T.DoubleType()),\n","    T.StructField(\"Symbol\", T.StringType()),\n","    T.StructField(\"timestamp\", T.TimestampType())\n","])\n","\n","df_stocks = (\n","    spark.read.format(\"csv\")\n","    .option(\"header\", \"true\")\n","    .schema(schema)\n","    .load(f\"{DATA_FOLDER}/csv/*/*/*.csv\")\n",")\n","\n","df_stocks.tail(8)"]},{"cell_type":"code","execution_count":null,"id":"dfed41be-e310-4089-8600-a737aa17a799","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# filter stocks to between min/max dates\n","\n","df_stocks = df_stocks.select(\"*\").where( \\\n","    f'timestamp >= \"{historicalBeginDate}\" and timestamp < \"{historicalEndDate}\"').sort(\"timestamp\")\n","\n","df_stocks.tail(8)"]},{"cell_type":"code","execution_count":null,"id":"1a5735f5","metadata":{},"outputs":[],"source":["# write to delta table -- this is the fastest method and should avoid conflicts\n","\n","df_stocks.write.format(\"delta\").mode(\"append\").option(\"inferSchema\",\"true\").saveAsTable(targetTableName)\n"]},{"cell_type":"code","execution_count":null,"id":"b370e070-f9c2-47e9-a7e3-d8be4090eda0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # a merge offers more flexibility to update the table, but risks a concurrency error\n","# # because the table is being updated by the eventstream\n","\n","# from delta.tables import *\n","\n","# def importIntoRaw(df):\n","\n","#   raw_table = DeltaTable.forName(spark, targetTableName)\n","\n","#   raw_table.alias('raw') \\\n","#     .merge(\n","#       df.alias('history'),\n","#       'raw.timestamp = history.timestamp and raw.symbol = history.symbol'\n","#     ) \\\n","#     .whenMatchedUpdate(set =\n","#         {\n","#           \"price\": \"history.price\"\n","#         }\n","#     ) \\\n","#     .whenNotMatchedInsert(values =\n","#       {\n","#           \"symbol\": \"history.symbol\"\n","#           ,\"price\": \"history.price\"\n","#           ,\"timestamp\": \"history.timestamp\"\n","#       }\n","#     ) \\\n","#     .execute()\n","\n","# Retries = 3\n","# IsSuccess = False\n","\n","# for i in range(Retries):\n","#   try:\n","#     importIntoRaw(df_stocks)\n","#     IsSuccess = True\n","#     print(f\"Completed merge\")\n","#     break\n","#   except delta.exceptions.ConcurrentAppendException as e:\n","#     print(f\"Concurrency error - please wait and try again: {e}\")\n","#     time.sleep(1)\n","#     continue\n","\n","# if not IsSuccess:\n","#   msg = f\"Failed to merge after {Retries} retries\"\n","#   raise SystemExit(msg)\n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
